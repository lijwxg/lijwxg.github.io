#+hugo_base_dir: ~/projects/blog/lijwxg

* kafka                                                               :kafka:
** DONE 使用 kafka, 基本环境和安装
   CLOSED: [2018-05-06 Sun 15:14]
   :properties:
   :export_file_name: kafka-start
   :end:
   介绍基本的安装和使用
   #+hugo: more
*** centos7 下的 javac 的问题

centos 默认安装的是 OpenJDK 没有 javac
如果需要 javac

#+begin_src shell
yum install java-devel
#+end_src

*** Kafka 管理器 kafka-manager 安装

运行环境要求:

1. Kafka0.8.1.1+
2. sbt0.13.X
3. Java 7+

*** kafka 安装

- 下载
- 解压

*** sbt 安装

#+begin_src shell
$ wget https://github.com/sbt/sbt/releases/download/v1.1.1/sbt-1.1.1.tgz
$ sudo mkdir /opt/scala/sbt                                             # 建立目录，解压文件到所建立目录
$ sudo tar -zxvf sbt-1.1.1.tgz -C /opt/scala/
$ cd /opt/scala/sbt
$ vim sbt                                                               # /*选定一个位置，建立启动sbt的脚本文本文件，如/opt/scala/sbt/ 目录下面新建文件名为sbt的文本文件*/
BT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"
java $SBT_OPTS -jar bin/sbt-launch.jar "$@"
$ chmod u+x sbt                                                         # /×修改sbt文件权限×/
$ vim ~/.bashrc                                                         # 配置PATH环境变量，保证在控制台中可以使用sbt命令
export PATH=/opt/scala/sbt/:$PATH                                       # /*在文件尾部添加如下代码后，保存退出*/
$ source ~/.bashrc                                                      # 使配置文件立刻生效
$ sbt sbt-version                                                       # 测试sbt是否安装成功
#+end_src

*** kafka-manager 包的编译

#+begin_src shell
$ git clone https://github.com/yahoo/kafka-manager
$ cd kafka-manager
$ ./sbt clean dist
$ cd tartget/universal
$ unzip kafka-manager-1.3.3.17.zip
$ vim conf/application.conf
kafka-manager.zkhosts="localhost:2181"
$ nohup bin/kafka-manager -Dconfig.file=conf/application.conf > /dev/null 2>&1 &
#+end_src


** DONE Kafka基本的概念                                             :bigdata:
   CLOSED: [2018-05-06 Sun 15:25]
   :properties:
   :export_file_name: kafka-basic
   :end:

在大数据中,使用了大量的数据. 关于数据, 我们有两个主要挑战:

- 第一个挑战是如何收集大量的数据,
- 第二个挑战是分析收集的数据
   #+hugo: more
*** Kafka 概述

为了克服这些挑战, 您必须需要一个消息系统. Kafka 专为分布式高吞吐量系统而设计. 作为一个更传统的消息代理的替代品, 与其它消息传递系统相比,Kafka 具有更好的吞吐量, 内置分区, 复制和固有的容错能力, 这使得它非常适合大规模消息处理应用系统.

**** 消息系统

消息系统负责将数据从一个应用程序传输到另一个应用程序, 因此应用程序可以专注于数据, 而不担心如何共享它. 分布式消息传递基于可靠消息队列的概念. 消息在客户端应用程序和消息传递系统之间异步排队. 有两种类型的消息模式可用:

- 一种是点对点
- 另一种是发布-订阅(pub-sub)消息系统.

大多数消息模式遵循 pub-sub

*点对点消息系统*
在点对点系统中, 消息被保留在队列中. 一个或多个消费者可以消耗队列中的消息, 但是特定消息只能由一个消费者消费. 一旦消费者读取队列中的消息, 它就从队列中消失. 该系统的典型事例是订单处理系统, 其中每个订单将由一个订单处理器处理,但多个订单处理器也可以同时工作

*发布-订阅消息系统*
在发布-订阅系统中, 消息被保留在主题中. 与点对点系统搞不通, 消费者可以订阅一个或多个主题并使用该主题的所有消息. 在发布-订阅系统中, 消息成为发布者, 消息使用者称为订阅者. 一个现实生活的例子是 Dish 电视, 它发布不同的渠道, 如运动, 电影, 音乐等, 任何人都可以订阅自己的频道集, 并获得他们订阅的频道时可用

**** 什么是 Kafka

Apache Kafka 是一个分布式发布-订阅消息系统和一个强大的队列, 可以处理大量的数据, 并使您能过将消息从一个端点传递到另一个端点. Kafka 适合离线和在线消息消费. Kafka 消息保留在磁盘上, 并在集群内复制以防止数据丢失. Kafka 构建在 ZooKeeper 同步服务之上, 它与 Apache Storm 和 Spark 非常好的集成, 用于实时流式数据分析

***** 好处

以下是使用 Kafka 的几个好处:

- 可靠性 Kafka 是分布式, 分区, 复制和容错的
- 可拓展性 Kafka 消息传递系统轻松缩放, 无需停机
- 耐用性 Kafka 使用分布式提交日志, 这以为这消息会尽可能地保留在磁盘上, 因此它是持久的.
- 性能 Kafka 对于发布和订阅消息都具有很高的吞吐量. 计时存储了许多 TB 的消息. 它也保持稳定的性能

Kafka 非常快, 并保证零停机和零数据丢失.

***** 用例

Kafka 可以在许多用例中使用. 其中一些列举如下:

- 指标 Kafka 通常用于操作监控数据. 这涉及聚合来自分布式应用程序的统计信息, 以产生操作数据的几种馈送.
- 日志聚合解决方案 Kafka 可用于跨组织从多个服务收集日志, 并使他们一标准格式提供给多个服务器.
- 流处理 流行的框架(Storm 和 Spark Streaming)从主体中读取数据, 对其进行处理, 并将处理后的数据写入新主题, 供用户和应用程序使用. Kafka 的耐久性在流处理的上下文中也非常有用.

*** Kafka 基础

在深入了解 kafka 之前, 需要了解主题, 经纪人, 生产者, 消费者等主要术语, 下图说明了主要术语

*** Kafka 集群

- *broker(代理)*:
- *ZooKeeper*
- *Producers(生产者)*
- *Consumers()*


** DONE Python调用Kafka
   CLOSED: [2018-12-29 Sat 16:01]
   :properties:
   :export_file_name: kafka-python
   :end:

运行 kafka 和 kafka-manager, 使用 python 连接 kafka

#+hugo: more

#+begin_src shell
tar -xzf kafka_2.11-1.0.1.tgz
cd kafka_2.11-1.0.1
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-fctor 1 --partitions 1 --topic test    # create a topic
bin/kafka-topics.sh --list --zookeeper localhost:2181                                                         # list topics
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test                                       # send some messages
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning                 # start a consumer
cd ../kafka-manager-1.3.3.17
nohup bin/kafka-manager -Dconfig.file=conf/application.conf > /dev/null 2>&1 &
#+end_src

kafka 的配置我这里就提及下重点 关于 host.name 这个参数

如果我们想远程消费这个 KAFKA 一定要把这个定义成能访问的 IP 比如我想在内网消费这个 KAFKA 就要用内网 IP 绑定

## Python 代码示例

#+begin_src python
#!/usr/bin/env python
# _*_coding:utf-8_*_
# __author__ = lijwxg@hotmail.com
# __file__ = kafka_test2.py
# __date__ = 2018/3/13
# __time__ = 9:50

from kafka import KafkaProducer
from kafka import KafkaConsumer
from kafka.errors import KafkaError
import json


class Kafka_producer():
    u"""
    使用kafka的生产模块
    """

    def __init__(self, kafkahost, kafkaport, kafkatopic):
        self.kafkaHost = kafkahost
        self.kafkaPort = kafkaport
        self.kafkatopic = kafkatopic
        self.producer = KafkaProducer(bootstrap_servers='{kafka_host}:{kafka_port}'.format(
            kafka_host=self.kafkaHost,
            kafka_port=self.kafkaPort
        ))

    def sendjsondata(self, params):
        try:
            parmas_message = json.dumps(params)
            producer = self.producer
            producer.send(self.kafkatopic, parmas_message.encode('utf-8'))
            producer.flush()                                                    # 此处如果没有,则发送的消息不会提交
        except KafkaError as e:
            print e


class Kafka_consumer():
    u"""
    使用Kafka—python的消费模块
    """

    def __init__(self, kafkahost, kafkaport, kafkatopic, groupid):
        self.kafkaHost = kafkahost
        self.kafkaPort = kafkaport
        self.kafkatopic = kafkatopic
        self.groupid = groupid
        self.consumer = KafkaConsumer(self.kafkatopic, group_id=self.groupid,
                                      bootstrap_servers='{kafka_host}:{kafka_port}'.format(
                                          kafka_host=self.kafkaHost,
                                          kafka_port=self.kafkaPort))

    def consume_data(self):
        try:
            for message in self.consumer:
                # print json.loads(message.value)
                yield message
        except KeyboardInterrupt, e:
            print e


def main():
    u"""
    测试consumer和producer
    :return:
    """
    # 测试生产模块
    producer = Kafka_producer("192.168.60.224", 9092, "test")
    for i in range(10):
        params = '{abetst}:{null}---' + str(i)
        producer.sendjsondata(params)
    # 测试消费模块
    # 消费模块的返回格式为ConsumerRecord(topic=u'ranktest', partition=0, offset=202, timestamp=None,
    # \timestamp_type=None, key=None, value='"{abetst}:{null}---0"', checksum=-1868164195,
    # \serialized_key_size=-1, serialized_value_size=21)
    # consumer = Kafka_consumer('127.0.0.1', 9092, "ranktest", 'test-python-ranktest')
    # message = consumer.consume_data()
    # for i in message:
    #     print i.value


if __name__ == '__main__':
    main()
#+end_src
